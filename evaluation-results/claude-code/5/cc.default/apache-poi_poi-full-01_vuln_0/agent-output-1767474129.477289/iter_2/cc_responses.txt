
================================================================================
Response 1
================================================================================
Now I understand the issue. Let me analyze the vulnerability and the failed patches:

## Analysis of the Vulnerability

The timeout vulnerability is caused by the LZW decompression algorithm in `CompressedRTF.java`. Looking at the code:

1. The `decompress` method in `CompressedRTF` reads a header with `compressedSize` and `decompressedSize` from the input stream
2. Then it calls `super.decompress(src, res)` which runs the LZW decompression in `LZWDecompresser`
3. The LZW decompression loop at line 157 (`while ((flag = src.read()) != -1)`) continues until the input stream is exhausted

**The root problem:** The decompression loop doesn't check when to stop based on the declared sizes. A malicious input can craft a small compressed stream that produces enormous output, or a stream that causes the decompressor to run indefinitely (a "zip bomb" or decompression bomb).

## Why the Failed Patches Didn't Work

**Patch 1** (adding `return` after uncompressed handling): This only fixed the case where uncompressed data was being processed twice (once via `IOUtils.copy` and once via `super.decompress`). But the timeout occurs during actual decompression, so this doesn't address the root cause.

**Patch 2** (limiting decompressed size with `LimitedOutputStream`): This approach tried to limit output, but:
1. The `LimitedOutputStream` only limits *bytes written* but doesn't stop the decompression loop
2. The LZW decompression in the parent class processes the entire input stream regardless
3. A malicious input with a small declared `decompressedSize` but crafted to produce more output would throw an exception but not stop processing
4. More critically, the timeout suggests the *processing itself* is taking too long, not just producing too much output

## The Correct Fix

The issue is that `LZWDecompresser.decompress()` reads until the input stream ends (`src.read() != -1`). We need to limit the input stream based on `compressedSize` (the header indicates how many compressed bytes to read). This prevents a malicious input from causing infinite decompression.

We should wrap the input stream to limit reads to `compressedSize - 12` bytes (minus the 12-byte header already read).

================================================================================
Response 2
================================================================================
Now let me show the complete patch:

================================================================================
Response 3
================================================================================
## Summary

### Why the Previous Patches Failed

**Patch 1** (adding `return` after uncompressed case):
- This only fixed the issue where uncompressed data was being processed twice (once via `IOUtils.copy` and then again via `super.decompress`)
- However, this doesn't address the timeout vulnerability in the *compressed* case

**Patch 2** (LimitedOutputStream with MAX_DECOMPRESSED_SIZE):
- This approach limited the *output* bytes written, but the vulnerability is in the *input processing*
- The LZW decompression loop in `LZWDecompresser.decompress()` at line 157 runs `while ((flag = src.read()) != -1)` - it continues until the input stream ends
- A malicious input can craft data that causes extremely long processing time regardless of output size
- The `decompressedSize` field comes from the input itself (attacker-controlled), so validating against it doesn't help if the attacker lies about the size
- The decompression loop would still run until the input stream is exhausted

### The New Fix

The new patch takes a **completely different approach** - limiting the **input stream** instead of the output:

1. **Added `BoundedInputStream`**: Uses Apache Commons IO's `BoundedInputStream` to wrap the source input stream
2. **Limits based on `compressedSize`**: The input is bounded to `compressedSize - 12` bytes (the declared compressed size minus the 16-byte header, of which 12 bytes remain after the first 4 bytes for compressedSize itself)
3. **Added `return` for uncompressed case**: Also included the fix from Patch 1 to avoid double-processing of uncompressed data
4. **Validates `compressedSize`**: Rejects negative compressed sizes

This approach works because:
- The decompression loop terminates when the bounded input stream returns -1 (end of allowed bytes)
- Even if an attacker provides a malicious stream designed to cause infinite loops, the bounded input stream will cut off after the declared number of compressed bytes
- The processing time becomes bounded by the declared compressed size, not the actual stream length

### The Patch

```diff
diff --git a/poi-scratchpad/src/main/java/org/apache/poi/hmef/CompressedRTF.java b/poi-scratchpad/src/main/java/org/apache/poi/hmef/CompressedRTF.java
index 46988b9ec..7c4b2e8d1 100644
--- a/poi-scratchpad/src/main/java/org/apache/poi/hmef/CompressedRTF.java
+++ b/poi-scratchpad/src/main/java/org/apache/poi/hmef/CompressedRTF.java
@@ -22,6 +22,7 @@ import java.io.InputStream;
 import java.io.OutputStream;
 import java.nio.charset.StandardCharsets;
 
+import org.apache.commons.io.input.BoundedInputStream;
 import org.apache.poi.util.IOUtils;
 import org.apache.poi.util.LZWDecompresser;
 import org.apache.poi.util.LittleEndian;
@@ -81,14 +82,21 @@ public final class CompressedRTF extends LZWDecompresser {
       if(compressionType == UNCOMPRESSED_SIGNATURE_INT) {
          // Nope, nothing fancy to do
          IOUtils.copy(src, res);
+         return;
       } else if(compressionType == COMPRESSED_SIGNATURE_INT) {
          // We need to decompress it below
       } else {
          throw new IllegalArgumentException("Invalid compression signature " + compressionType);
       }
 
+      // Limit the input stream to the declared compressed size (minus 12-byte header already read)
+      // This prevents malicious inputs from causing infinite decompression loops
+      int maxCompressedBytes = compressedSize - 12;
+      if (maxCompressedBytes < 0) {
+         throw new IllegalArgumentException("Invalid compressed size: " + compressedSize);
+      }
+      InputStream boundedSrc = BoundedInputStream.builder().setInputStream(src).setMaxCount(maxCompressedBytes).get();
+
       // Have it processed
-      super.decompress(src, res);
+      super.decompress(boundedSrc, res);
    }
```
