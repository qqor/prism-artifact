
================================================================================
Response 1
================================================================================
Now I understand the issue. Looking at the crash log and code:

1. The crash occurs at `LZWInputStream.initializeTables(LZWInputStream.java:186)` which is the line `prefixes = new int[maxTableSize];`
2. The `maxCodeSize` is read from the 3rd byte of the input file with a mask of `0x1f` (5 bits), allowing values from 0 to 31
3. When `maxCodeSize` is a very large value (like 30 or 31), `1 << maxCodeSize` creates a huge table size (2^30 or 2^31 entries), causing OOM

The fix should validate `maxCodeSize` against a reasonable maximum. According to the Unix compress (.Z) format specification, the maximum code size should be between 9 and 16 bits. Let me check if there's existing validation:

================================================================================
Response 2
================================================================================
Now I see the issue clearly. The problem is:

1. `ZCompressorInputStream` reads `maxCodeSize` from the input file with `maxCodeSize = thirdByte & MAX_CODE_SIZE_MASK` (0x1f = 31)
2. It passes `-1` as the memory limit when called via `ZCompressorInputStream(inputStream)` (no memory limit)
3. In `initializeTables(maxCodeSize, memoryLimiKiB)`, when `memoryLimiKiB` is `-1`, the memory check is skipped entirely
4. Then `initializeTables(maxCodeSize)` is called which allocates `1 << maxCodeSize` arrays

The Unix compress format specification says the maximum code size should be 16 bits (though some implementations support up to 18). The code should validate that `maxCodeSize` is within a reasonable range.

The fix should be in `ZCompressorInputStream.java` to validate the `maxCodeSize` value before using it. Looking at the standard, the minimum is 9 and maximum is typically 16:

================================================================================
Response 3
================================================================================
The patch has been applied. Let me verify the final state of the file:

================================================================================
Response 4
================================================================================
The patch has been successfully applied to `ZCompressorInputStream.java`. Here's a summary of the fix:

## Summary of the Fix

**Problem:** The `ZCompressorInputStream` class was vulnerable to an Out of Memory (OOM) attack via a maliciously crafted `.Z` file. The `maxCodeSize` value was read from the input file (5 bits, allowing values 0-31) and used directly to allocate arrays of size `1 << maxCodeSize`. A malicious file with `maxCodeSize = 31` would try to allocate arrays of 2^31 entries, causing OOM.

**Root Cause:** The code at line 82 extracted `maxCodeSize` from the input without validation:
```java
maxCodeSize = thirdByte & MAX_CODE_SIZE_MASK;  // MAX_CODE_SIZE_MASK = 0x1f (31)
```

**Fix Applied:** Added validation to ensure `maxCodeSize` is within the valid range for the Unix compress format (9 to 16 bits):

1. Added a constant `MAX_CODE_SIZE_UPPER_BOUND = 16` (line 40)
2. Added validation check before using `maxCodeSize` (lines 85-87):
```java
if (maxCodeSize < DEFAULT_CODE_SIZE || maxCodeSize > MAX_CODE_SIZE_UPPER_BOUND) {
    throw new IOException("Invalid .Z format: maxCodeSize is " + maxCodeSize + ", must be in range " + DEFAULT_CODE_SIZE + " to " + MAX_CODE_SIZE_UPPER_BOUND);
}
```

**Why 16?** The Unix compress specification defines the maximum code size as 16 bits. While some implementations support up to 18 bits, 16 bits is the standard maximum and allocates at most 2^16 = 65536 entries (about 384KB of memory), which is reasonable.
